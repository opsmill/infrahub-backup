---
title: Understanding backup strategies for Infrahub
---

This topic explores backup strategies for Infrahub deployments, examining the design decisions, trade-offs, and considerations that shape an effective backup approach. Understanding these concepts helps you make informed decisions about protecting your infrastructure data.

## Introduction

Backup strategies for Infrahub go beyond simply copying files. The tool orchestrates a complex dance of stopping services, ensuring consistency, capturing multiple data stores, and minimizing downtime. This explanation reveals why certain approaches were chosen and how they work together to provide reliable data protection.

## The backup philosophy

### Why comprehensive backups matter

Infrahub's architecture combines multiple data stores that must remain synchronized:

- **Neo4j graph database**: Stores the infrastructure model, relationships, and version history
- **PostgreSQL database**: Manages task execution, workflow states, and scheduling
- **File artifacts**: Contains templates, scripts, and uploaded files

These components are interdependent. A network device configuration might reference a Jinja2 template (artifact), which was generated by a task (PostgreSQL), based on data in the graph (Neo4j). Backing up only one component would leave you with an inconsistent state.

### The consistency challenge

Consider what happens during normal operation:

1. A user commits a change to the graph database
2. This triggers a task that generates new configurations
3. The task updates its status in PostgreSQL
4. Generated artifacts are stored to disk

If we backed up each component at different times, we might capture the graph change but miss the task status, or vice versa. This is why infrahubops takes a different approach.

## How infrahubops ensures consistency

### The stop-the-world approach

Infrahubops uses a "stop-the-world" strategy during backup:

```text
Time →
────────────────────────────────────────────
Normal    | Stop      | Backup        | Resume
Operation | Services  | All Data      | Services
────────────────────────────────────────────
```

This approach guarantees:

- No data changes during backup
- All components reflect the same point in time
- No partially completed transactions

The trade-off is temporary unavailability, typically 30-60 seconds for most deployments.

### Alternative approach: Online backups

Some systems offer online backups that don't require stopping services. Why doesn't infrahubops use this approach?

**Neo4j considerations**:

- Online backups require Enterprise Edition licensing
- Community Edition only supports offline backups
- Even online backups can impact performance

**PostgreSQL considerations**:

- pg_dump is already non-blocking
- But we need Neo4j stopped anyway for consistency

**Design decision**: By accepting brief downtime, infrahubops works with all Neo4j editions and guarantees perfect consistency.

## Component-specific strategies

### Neo4j database backup

Neo4j stores data as a property graph with ACID transactions. The backup strategy leverages Neo4j's built-in tools:

```bash
neo4j-admin database backup --to-path=/backup/location
```

This command:

1. Creates a consistent snapshot of the entire graph
2. Includes indexes and constraints
3. Preserves transaction logs for point-in-time recovery
4. Optionally includes user/role metadata

**Metadata options explained**:

- `all`: Complete backup including security settings
- `users`: Only user accounts (useful for migrations)
- `roles`: Only role definitions (for permission templates)
- `none`: Minimal backup, security reconstructed separately

### PostgreSQL task database

The task manager uses PostgreSQL to track:

- Workflow definitions and schedules
- Task execution history
- Flow run states and logs

Backup approach uses `pg_dump`:

```bash
pg_dump --clean --if-exists --no-owner
```

These flags ensure:

- `--clean`: Drops existing objects before restore
- `--if-exists`: Prevents errors if objects don't exist
- `--no-owner`: Allows restore by different user

### File artifacts

Artifacts include:

- Jinja2 templates
- Python transforms
- Generated configurations
- Uploaded documentation

Simple file copy preserves:

- Directory structure
- File permissions
- Modification timestamps

## Backup sizing and performance

### Understanding backup growth

Backup sizes typically follow this pattern:

```bash
Component       | Initial | After 6 months | After 1 year
----------------|---------|----------------|-------------
Neo4j           | 100 MB  | 500 MB        | 1 GB
PostgreSQL      | 50 MB   | 200 MB        | 400 MB
Artifacts       | 10 MB   | 100 MB        | 300 MB
Compressed      | 50 MB   | 250 MB        | 500 MB
```

Factors affecting size:

- Number of managed devices
- Frequency of changes
- History retention settings
- Task execution volume

### Compression strategies

Infrahubops uses tar with gzip compression:

```bash
Uncompressed → tar → gzip → Final archive
   2 GB         2 GB   500 MB
```

Typical compression ratios:

- Neo4j data: 70-80% reduction
- SQL dumps: 80-90% reduction
- Text artifacts: 60-70% reduction
- Binary files: 10-20% reduction

## Retention strategies

### The 3-2-1 rule

A robust backup strategy follows the 3-2-1 rule:

- **3** copies of important data
- **2** different storage media types
- **1** offsite copy

Applied to Infrahub:

```bash
Primary → Daily Backup → Weekly Archive → Cloud Storage
(Live)    (Local Disk)    (Network NAS)    (S3/Azure)
```

### Retention schedule patterns

**Simple retention**:

- Keep last 7 daily backups
- Keep last 4 weekly backups
- Keep last 12 monthly backups

**Grandfather-father-son**:

- Daily backups (son): 7 days
- Weekly backups (father): 4 weeks
- Monthly backups (grandfather): 12 months

**Progressive thinning**:

- All backups for 7 days
- Daily backups for 30 days
- Weekly backups for 3 months
- Monthly backups forever

### Storage calculations

Estimate storage needs:

```bash
Daily backup size: 500 MB
Daily growth rate: 10 MB

Month 1: 7 × 500 MB = 3.5 GB
Month 3: 7 × 800 MB = 5.6 GB
Month 6: 7 × 1.3 GB = 9.1 GB

With weekly archives:
Month 6: (7 daily × 1.3 GB) + (4 weekly × 1.2 GB) = 13.9 GB
```

## Recovery time objectives

### Understanding RTO and RPO

**Recovery Time Objective (RTO)**: How quickly you need to restore service
**Recovery Point Objective (RPO)**: How much data loss is acceptable

Different strategies for different requirements:

| Scenario | RPO | RTO | Strategy |
|----------|-----|-----|----------|
| Development | 24 hours | 4 hours | Daily backup |
| Staging | 4 hours | 1 hour | 4-hour snapshots |
| Production | 1 hour | 15 minutes | Hourly + hot standby |

### Optimizing recovery time

Factors affecting recovery speed:

1. **Backup size**: Larger backups take longer to restore
2. **Storage location**: Local is faster than remote
3. **Decompression**: CPU-bound operation
4. **Database import**: I/O intensive
5. **Service startup**: Container initialization

Optimization techniques:

- Keep recent backups on fast local storage
- Use incremental backups for large datasets
- Pre-stage backups in recovery environment
- Practice recovery procedures regularly

## Mental models for backup planning

### The insurance analogy

Think of backups like insurance:

- **Premium** = Storage and compute costs
- **Deductible** = Data loss (RPO)
- **Coverage** = What's protected
- **Claim time** = Recovery time (RTO)

Higher premiums (more frequent backups, multiple copies) reduce your deductible (less data loss) and claim time (faster recovery).

### The time machine model

Backups are time machines for your data:

- Each backup is a snapshot in time
- You can only travel to times you've saved
- The machine takes time to warm up (restore)
- Some details might be fuzzy (metadata options)

### The safety net pyramid

```text
        Daily
       /      \
      /        \
     /  Weekly  \
    /            \
   /   Monthly    \
  /                \
 /     Yearly       \
/____________________\
```

Each level catches failures the level above missed:

- Daily: Catches immediate mistakes
- Weekly: Protects against daily corruption
- Monthly: Guards against systematic issues
- Yearly: Compliance and historical records

## Design decisions and trade-offs

### Why not incremental backups?

Incremental backups only store changes since the last backup. Infrahubops uses full backups instead. Why?

**Advantages of full backups**:

- Simple recovery (single file)
- No dependency chains
- Parallel backup/restore possible
- Easy to verify and test

**Trade-offs**:

- More storage space required
- Longer backup windows
- More network bandwidth

For most Infrahub deployments, the simplicity and reliability of full backups outweighs the storage costs.

### Container stopping order

Infrahubops stops containers in specific order:

1. Application servers (prevent new changes)
2. Task workers (complete in-flight tasks)
3. Cache and message queue (flush buffers)
4. Databases (last to stop, first to start)

This sequence minimizes the chance of data inconsistency.

## Further reading

- [How to backup your Infrahub instance](../guides/backup-instance.mdx)
- [How to restore from backup](../guides/restore-backup.mdx)
- [About deployment environments](./deployment-environments.mdx)
- [CLI command reference](../reference/commands.mdx)
